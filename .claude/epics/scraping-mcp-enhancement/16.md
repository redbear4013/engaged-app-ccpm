# Task 004: Data Pipeline & Quality Controls

## Metadata
```yaml
epic: scraping-mcp-enhancement
task_id: 004
title: Data Pipeline & Quality Controls
status: pending
priority: high
size: L
effort_hours: 16-20
parallel: false
depends_on: [14]
assignee: unassigned
created: 2025-10-01
updated: 2025-10-01
```

## Objective
Implement comprehensive data quality controls including field normalization, event classification with confidence scoring, multi-tier deduplication, and revision tracking with administrative review workflows.

## Scope

### In Scope
- Field normalization logic (dates, prices, venues)
- Event classification rules with confidence scoring
- Deduplication engine (3-tier matching: exact, fuzzy, semantic)
- Revision tracking with automatic diff generation
- Admin review queue for low-confidence events
- Data quality metrics and reporting

### Out of Scope
- Machine learning-based classification (future enhancement)
- Manual data entry interface
- Bulk edit operations
- Historical data migration

## Technical Approach

### 1. Field Normalization Layer
```typescript
// src/lib/scraping/normalizers/index.ts
interface NormalizationPipeline {
  normalizeDates(raw: string[]): DateRange;
  normalizePrices(raw: string[]): PriceInfo;
  normalizeVenue(raw: VenueData): NormalizedVenue;
  normalizeCategories(raw: string[]): Category[];
}
```

**Date Normalization:**
- Parse multiple date formats (ISO, locale-specific, relative)
- Handle recurring events (weekly, monthly patterns)
- Timezone conversion and DST handling
- Date range extraction and validation

**Price Normalization:**
- Extract numeric values from text ("$15-$25", "Free", "Donation")
- Currency detection and conversion
- Price tier classification (free, budget, premium)
- Early bird vs regular pricing extraction

**Venue Normalization:**
- Address standardization via geocoding API
- Duplicate venue detection and merging
- Capacity estimation from venue type
- Accessibility info extraction

### 2. Event Classification System
```typescript
// src/lib/scraping/classifiers/event-classifier.ts
interface ClassificationResult {
  categories: Category[];
  confidence: number; // 0.0 - 1.0
  signals: ClassificationSignal[];
  needsReview: boolean;
}

interface ClassificationSignal {
  type: 'keyword' | 'venue_type' | 'price_range' | 'time_pattern';
  category: Category;
  weight: number;
}
```

**Classification Rules:**
- Keyword-based primary classification
- Venue type inference (theater → "Performing Arts")
- Price range correlation (free → "Community", >$50 → "Premium")
- Time pattern analysis (weekday evening → "Nightlife")
- Multi-label support with confidence scoring

**Confidence Thresholds:**
- High (≥0.8): Auto-approve
- Medium (0.5-0.8): Flag for review
- Low (<0.5): Reject or manual review required

### 3. Deduplication Engine

**Tier 1: Exact Matching**
- SHA-256 hash of normalized fields
- Detects identical events from multiple sources
- O(1) lookup performance

**Tier 2: Fuzzy Matching**
```typescript
interface FuzzyMatchCriteria {
  titleSimilarity: number; // Levenshtein distance
  venueSimilarity: number; // Address + name matching
  dateOverlap: boolean;    // Same day/time window
  threshold: number;       // Combined score threshold
}
```

**Tier 3: Semantic Matching**
- Embeddings-based similarity (future with LLM integration)
- Cross-reference external event IDs
- Social media link matching

**Merge Strategy:**
- Prioritize official sources over aggregators
- Preserve richest data (most fields populated)
- Track merge history for audit trail

### 4. Revision Tracking System
```typescript
// src/lib/scraping/revisions/tracker.ts
interface EventRevision {
  event_id: string;
  revision_number: number;
  changed_fields: FieldDiff[];
  change_reason: 'scrape_update' | 'admin_edit' | 'merge';
  changed_at: Date;
  changed_by: string; // source or admin user
}

interface FieldDiff {
  field: string;
  old_value: any;
  new_value: any;
  confidence_delta?: number;
}
```

**Diff Generation:**
- Deep object comparison
- Ignore cosmetic changes (whitespace, case)
- Track confidence score changes
- Highlight material updates (date, price, venue changes)

**Revision Policies:**
- Auto-approve high-confidence updates
- Require admin review for material changes
- Reject downgrades (less complete data)
- Version history retention (last 10 revisions)

### 5. Admin Review Queue
```typescript
// src/lib/scraping/review/queue.ts
interface ReviewQueueItem {
  id: string;
  event_id: string;
  review_reason: ReviewReason;
  severity: 'low' | 'medium' | 'high';
  auto_resolve_at?: Date; // Auto-approve after 48h
  assigned_to?: string;
  created_at: Date;
}

type ReviewReason =
  | 'low_confidence_classification'
  | 'duplicate_uncertain'
  | 'material_field_change'
  | 'new_source_first_event'
  | 'manual_flag';
```

**Review Workflow:**
1. Items added to queue with severity scoring
2. Email/Slack notification for high-severity items
3. Admin dashboard displays pending reviews
4. Bulk approve/reject actions
5. Auto-resolve after timeout with audit log

## Database Schema

```sql
-- Revision tracking
CREATE TABLE event_revisions (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  event_id UUID REFERENCES events(id) ON DELETE CASCADE,
  revision_number INT NOT NULL,
  changed_fields JSONB NOT NULL,
  change_reason TEXT NOT NULL,
  changed_by TEXT NOT NULL,
  created_at TIMESTAMPTZ DEFAULT NOW(),
  UNIQUE(event_id, revision_number)
);

CREATE INDEX idx_revisions_event ON event_revisions(event_id);
CREATE INDEX idx_revisions_created ON event_revisions(created_at DESC);

-- Review queue
CREATE TABLE review_queue (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  event_id UUID REFERENCES events(id) ON DELETE CASCADE,
  review_reason TEXT NOT NULL,
  severity TEXT NOT NULL CHECK (severity IN ('low', 'medium', 'high')),
  metadata JSONB,
  auto_resolve_at TIMESTAMPTZ,
  assigned_to UUID REFERENCES admin_users(id),
  resolved_at TIMESTAMPTZ,
  resolved_by UUID REFERENCES admin_users(id),
  resolution TEXT CHECK (resolution IN ('approved', 'rejected', 'modified')),
  created_at TIMESTAMPTZ DEFAULT NOW()
);

CREATE INDEX idx_queue_pending ON review_queue(created_at DESC)
  WHERE resolved_at IS NULL;
CREATE INDEX idx_queue_severity ON review_queue(severity, created_at DESC);

-- Deduplication tracking
CREATE TABLE event_duplicates (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  canonical_event_id UUID REFERENCES events(id) ON DELETE CASCADE,
  duplicate_event_id UUID REFERENCES events(id) ON DELETE CASCADE,
  match_tier TEXT CHECK (match_tier IN ('exact', 'fuzzy', 'semantic')),
  match_confidence DECIMAL(3,2),
  merged_at TIMESTAMPTZ DEFAULT NOW(),
  merged_by TEXT NOT NULL
);
```

## Deliverables

### Code Artifacts
- [ ] `src/lib/scraping/normalizers/` - Field normalization modules
  - `date-normalizer.ts`
  - `price-normalizer.ts`
  - `venue-normalizer.ts`
  - `category-normalizer.ts`
- [ ] `src/lib/scraping/classifiers/event-classifier.ts` - Classification engine
- [ ] `src/lib/scraping/deduplication/` - Dedup engine
  - `exact-matcher.ts`
  - `fuzzy-matcher.ts`
  - `merge-strategy.ts`
- [ ] `src/lib/scraping/revisions/tracker.ts` - Revision tracking
- [ ] `src/lib/scraping/review/queue.ts` - Review queue manager

### Database Migrations
- [ ] `migrations/009_revision_tracking.sql`
- [ ] `migrations/010_review_queue.sql`
- [ ] `migrations/011_event_duplicates.sql`

### Tests
- [ ] `normalizers.test.ts` - All normalization functions
- [ ] `classifier.test.ts` - Classification accuracy tests
- [ ] `deduplication.test.ts` - Match tier coverage
- [ ] `revisions.test.ts` - Diff generation correctness
- [ ] `review-queue.test.ts` - Workflow state transitions

### Documentation
- [ ] `docs/data-quality.md` - Quality control overview
- [ ] `docs/classification-rules.md` - Category rules reference
- [ ] `docs/admin-review-guide.md` - Review workflow guide

## Acceptance Criteria

### Functional Requirements
- [ ] Date parser handles 10+ common formats with 95%+ accuracy
- [ ] Price extraction supports free/donation/range/tiered pricing
- [ ] Venue normalizer reduces duplicates by 80%+ via geocoding
- [ ] Classifier achieves 85%+ accuracy on test dataset
- [ ] Exact dedup matches 100% identical events
- [ ] Fuzzy dedup catches 90%+ near-duplicates with <5% false positives
- [ ] Revision diffs highlight material changes only
- [ ] Review queue auto-resolves low-severity items after 48h

### Non-Functional Requirements
- [ ] Normalization pipeline processes 1000 events/minute
- [ ] Classification runs in <50ms per event
- [ ] Deduplication scales to 100k+ events
- [ ] Review queue supports 10k+ pending items
- [ ] All operations logged for audit trail

## Dependencies
- **Blocks**: None
- **Blocked By**: Task 002 (Source Adapters) - needs standardized event data
- **Related**: Task 006 (Monitoring) - quality metrics integration

## Risks & Mitigation

| Risk | Impact | Likelihood | Mitigation |
|------|--------|------------|------------|
| Classification accuracy below target | High | Medium | Iterative tuning + manual review fallback |
| Fuzzy matching false positives | Medium | Medium | Adjustable thresholds + admin override |
| Geocoding API costs exceed budget | Medium | Low | Cache results + batch requests |
| Review queue backlog overwhelms admins | High | Medium | Auto-approval policies + severity filtering |

## Timeline
- **Day 1-2**: Normalization layer implementation
- **Day 3-4**: Classification system + rule tuning
- **Day 5-7**: Deduplication engine (all tiers)
- **Day 8-9**: Revision tracking + diff generation
- **Day 10**: Review queue + workflow logic
- **Day 11-12**: Integration testing + performance optimization
- **Day 13**: Documentation + handoff

## Success Metrics
- Data quality score: ≥90% (weighted: completeness, accuracy, consistency)
- Duplicate reduction: ≥80% vs baseline
- Classification confidence: ≥85% high-confidence events
- Review queue throughput: <24h median resolution time
- Admin workload: <10% events require manual review
